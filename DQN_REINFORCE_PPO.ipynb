{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dPXJWr7e6Z1D"
      },
      "source": [
        "# DQN\\REINFORCE\\PPO Part of Project"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYLWO5Lk6Z1E"
      },
      "source": [
        "## Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mJHUrZov6Z1E"
      },
      "outputs": [],
      "source": [
        "import collections\n",
        "import gymnasium as gym\n",
        "import itertools\n",
        "import numpy as np\n",
        "\n",
        "# from numpy.typing import NDArray\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import random\n",
        "import optuna\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "from typing import Callable, cast, List, Tuple, Union\n",
        "from collections import deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zDCYljLB6Z1F"
      },
      "outputs": [],
      "source": [
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tGS0wIT6Z1F"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQuZMDjJ6Z1F"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Video\n",
        "from ipywidgets import interact"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jlOztYgj6Z1F"
      },
      "outputs": [],
      "source": [
        "import warnings\n",
        "\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yQ2Wch4P6Z1G"
      },
      "outputs": [],
      "source": [
        "sns.set_context(\"talk\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hGGAqQUp6Z1G"
      },
      "outputs": [],
      "source": [
        "FIGS_DIR = Path(\"figs/\") / \"project\"       # Where to save figures (.gif or .mp4 files)\n",
        "PLOTS_DIR = Path(\"figs/\") / \"project\"      # Where to save plots (.png or .svg files)\n",
        "MODELS_DIR = Path(\"models/\") / \"project\"   # Where to save models (.pth files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sit-jO6Q6Z1G"
      },
      "outputs": [],
      "source": [
        "if not FIGS_DIR.exists():\n",
        "    FIGS_DIR.mkdir(parents=True)\n",
        "if not PLOTS_DIR.exists():\n",
        "    PLOTS_DIR.mkdir(parents=True)\n",
        "if not MODELS_DIR.exists():\n",
        "    MODELS_DIR.mkdir(parents=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hl9RKI_S6Z1G"
      },
      "outputs": [],
      "source": [
        "def video_selector(file_path: List[Path]):\n",
        "    return Video(file_path, embed=True, html_attributes=\"controls autoplay loop\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4bf-Vz06Z1G"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93ejnDb66Z1G"
      },
      "outputs": [],
      "source": [
        "DEFAULT_NUMBER_OF_TRAININGS = 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kt9cPWYH6Z1G"
      },
      "source": [
        "## Discrete Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9uP8gOOy6Z1G"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGGFqHtR6Z1G"
      },
      "outputs": [],
      "source": [
        "LL_observation_dim = env.observation_space._shape[0]\n",
        "LL_action_number = env.action_space.n\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tiUdpUmx6Z1G"
      },
      "source": [
        "### Random Policy Baseline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHIAOoP_6Z1G"
      },
      "outputs": [],
      "source": [
        "def random_baseline(num_episodes):\n",
        "    episode_reward_list = []\n",
        "    for episode in range(num_episodes):\n",
        "        episode_reward = 0.0\n",
        "        observation, info = env.reset()\n",
        "        end = False\n",
        "        while not end:\n",
        "            action = env.action_space.sample()\n",
        "            observation, reward, terminated, truncated, info = env.step(action)\n",
        "            episode_reward += reward\n",
        "            end = terminated or truncated\n",
        "        episode_reward_list.append(episode_reward)\n",
        "    return episode_reward_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GFRdkTXG6Z1G"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "baseline_reward = random_baseline(num_episodes = 200)\n",
        "baseline_index = np.arange(200)\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSOye-6g6Z1H"
      },
      "outputs": [],
      "source": [
        "print(np.mean(baseline_reward),np.max(baseline_reward))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Akr9iSY76Z1H"
      },
      "outputs": [],
      "source": [
        "g = sns.relplot(\n",
        "    x= baseline_index,\n",
        "    y= baseline_reward,\n",
        "    kind=\"line\",\n",
        "    height=7,\n",
        "    aspect=2,\n",
        "    alpha=0.5,\n",
        ")\n",
        "plt.savefig(PLOTS_DIR / \"project_random_baseline.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YGaWFpGU6Z1H"
      },
      "outputs": [],
      "source": [
        "VIDEO_PREFIX_INITIALIZATION = \"project_random_policy\"\n",
        "\n",
        "(FIGS_DIR / f\"{VIDEO_PREFIX_INITIALIZATION}-episode-0.mp4\").unlink(missing_ok=True)\n",
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "env = gym.wrappers.RecordVideo(env, video_folder=str(FIGS_DIR), name_prefix=VIDEO_PREFIX_INITIALIZATION)\n",
        "observation, info = env.reset()\n",
        "end = False\n",
        "while not end:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, terminated, truncated, info = env.step(action)\n",
        "    end = terminated or truncated\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EKsfIj9o6Z1H"
      },
      "outputs": [],
      "source": [
        "Video(\n",
        "    FIGS_DIR / f\"{VIDEO_PREFIX_INITIALIZATION}-episode-0.mp4\",\n",
        "    embed=True,\n",
        "    html_attributes=\"controls autoplay loop\",\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qTg5f_BZ6Z1H"
      },
      "source": [
        "### DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idWf_If76Z1H"
      },
      "outputs": [],
      "source": [
        "class LinearNet(torch.nn.Module):\n",
        "    def __init__(self, dim_observation: int, n_action: int, hidden_dim: int, n_layer: int):\n",
        "        super().__init__()\n",
        "        self.dim_observation = dim_observation\n",
        "        self.n_action = n_action\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layer = n_layer\n",
        "        assert n_layer > 0\n",
        "        if n_layer == 1:\n",
        "            self.layer = torch.nn.Linear(dim_observation, n_action)\n",
        "        else:\n",
        "            layers = [torch.nn.Linear(dim_observation,hidden_dim),torch.nn.ReLU()]\n",
        "            for i in range(n_layer-2):\n",
        "                layers.append(torch.nn.Linear(hidden_dim,hidden_dim))\n",
        "                layers.append(torch.nn.ReLU())\n",
        "            layers.append(torch.nn.Linear(hidden_dim, n_action))\n",
        "            self.layer = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        output_tensor = self.layer(x)\n",
        "        return output_tensor\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SjgGFBzQ6Z1H"
      },
      "outputs": [],
      "source": [
        "class EpsilonGreedy:\n",
        "    \"\"\"\n",
        "    An Epsilon-Greedy policy.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    epsilon : float\n",
        "        The initial probability of choosing a random action.\n",
        "    epsilon_min : float\n",
        "        The minimum probability of choosing a random action.\n",
        "    epsilon_decay : float\n",
        "        The decay rate for the epsilon value after each action.\n",
        "    env : gym.Env\n",
        "        The environment in which the agent is acting.\n",
        "    q_network : torch.nn.Module\n",
        "        The Q-Network used to estimate action values.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    __call__(state: np.ndarray) -> np.int64\n",
        "        Select an action for the given state using the epsilon-greedy policy.\n",
        "    decay_epsilon()\n",
        "        Decay the epsilon value after each action.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        epsilon_start: float,\n",
        "        epsilon_min: float,\n",
        "        epsilon_decay: float,\n",
        "        env: gym.Env,\n",
        "        q_network: torch.nn.Module,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a new instance of EpsilonGreedy.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        epsilon_start : float\n",
        "            The initial probability of choosing a random action.\n",
        "        epsilon_min : float\n",
        "            The minimum probability of choosing a random action.\n",
        "        epsilon_decay : float\n",
        "            The decay rate for the epsilon value after each episode.\n",
        "        env : gym.Env\n",
        "            The environment in which the agent is acting.\n",
        "        q_network : torch.nn.Module\n",
        "            The Q-Network used to estimate action values.\n",
        "        \"\"\"\n",
        "        self.epsilon = epsilon_start\n",
        "        self.epsilon_min = epsilon_min\n",
        "        self.epsilon_decay = epsilon_decay\n",
        "        self.env = env\n",
        "        self.q_network = q_network\n",
        "\n",
        "    def __call__(self, state: np.ndarray) -> np.int64:\n",
        "        \"\"\"\n",
        "        Select an action for the given state using the epsilon-greedy policy.\n",
        "\n",
        "        If a randomly chosen number is less than epsilon, a random action is chosen.\n",
        "        Otherwise, the action with the highest estimated action value is chosen.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : np.ndarray\n",
        "            The current state of the environment.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        np.int64\n",
        "            The chosen action.\n",
        "        \"\"\"\n",
        "\n",
        "        if random.random() < self.epsilon:\n",
        "            action = np.random.choice(self.env.action_space.n)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "                q_values = self.q_network(state_tensor)\n",
        "\n",
        "                action = torch.argmax(q_values).item()\n",
        "\n",
        "        return action\n",
        "\n",
        "    def decay_epsilon(self):\n",
        "        \"\"\"\n",
        "        Decay the epsilon value after each episode.\n",
        "\n",
        "        The new epsilon value is the maximum of `epsilon_min` and the product of the current\n",
        "        epsilon value and `epsilon_decay`.\n",
        "        \"\"\"\n",
        "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOzuqIVv6Z1H"
      },
      "outputs": [],
      "source": [
        "class MinimumExponentialLR(torch.optim.lr_scheduler.ExponentialLR):\n",
        "    def __init__(\n",
        "        self,\n",
        "        optimizer: torch.optim.Optimizer,\n",
        "        lr_decay: float,\n",
        "        last_epoch: int = -1,\n",
        "        min_lr: float = 1e-6,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Initialize a new instance of MinimumExponentialLR.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        optimizer : torch.optim.Optimizer\n",
        "            The optimizer whose learning rate should be scheduled.\n",
        "        lr_decay : float\n",
        "            The multiplicative factor of learning rate decay.\n",
        "        last_epoch : int, optional\n",
        "            The index of the last epoch. Default is -1.\n",
        "        min_lr : float, optional\n",
        "            The minimum learning rate. Default is 1e-6.\n",
        "        \"\"\"\n",
        "        self.min_lr = min_lr\n",
        "        super().__init__(optimizer, lr_decay, last_epoch=-1)\n",
        "\n",
        "    def get_lr(self) -> List[float]:\n",
        "        \"\"\"\n",
        "        Compute learning rate using chainable form of the scheduler.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        List[float]\n",
        "            The learning rates of each parameter group.\n",
        "        \"\"\"\n",
        "        return [\n",
        "            max(base_lr * self.gamma**self.last_epoch, self.min_lr)\n",
        "            for base_lr in self.base_lrs\n",
        "        ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDa9_wKA6Z1H"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    \"\"\"\n",
        "    A Replay Buffer.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    buffer : collections.deque\n",
        "        A double-ended queue where the transitions are stored.\n",
        "\n",
        "    Methods\n",
        "    -------\n",
        "    add(state: np.ndarray, action: np.int64, reward: float, next_state: np.ndarray, done: bool)\n",
        "        Add a new transition to the buffer.\n",
        "    sample(batch_size: int) -> Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
        "        Sample a batch of transitions from the buffer.\n",
        "    __len__()\n",
        "        Return the current size of the buffer.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, capacity: int):\n",
        "        \"\"\"\n",
        "        Initializes a ReplayBuffer instance.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        capacity : int\n",
        "            The maximum number of transitions that can be stored in the buffer.\n",
        "        \"\"\"\n",
        "        self.buffer: collections.deque = collections.deque(maxlen=capacity)\n",
        "\n",
        "    def add(\n",
        "        self,\n",
        "        state: np.ndarray,\n",
        "        action: np.int64,\n",
        "        reward: float,\n",
        "        next_state: np.ndarray,\n",
        "        done: bool,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Add a new transition to the buffer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        state : np.ndarray\n",
        "            The state vector of the added transition.\n",
        "        action : np.int64\n",
        "            The action of the added transition.\n",
        "        reward : float\n",
        "            The reward of the added transition.\n",
        "        next_state : np.ndarray\n",
        "            The next state vector of the added transition.\n",
        "        done : bool\n",
        "            The final state of the added transition.\n",
        "        \"\"\"\n",
        "        self.buffer.append((state, action, reward, next_state, done))\n",
        "\n",
        "    def sample(\n",
        "        self, batch_size: int\n",
        "    ) -> Tuple[np.ndarray, Tuple[int], Tuple[float], np.ndarray, Tuple[bool]]:\n",
        "        \"\"\"\n",
        "        Sample a batch of transitions from the buffer.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        batch_size : int\n",
        "            The number of transitions to sample.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        Tuple[np.ndarray, float, float, np.ndarray, bool]\n",
        "            A batch of `batch_size` transitions.\n",
        "        \"\"\"\n",
        "        # Here, `random.sample(self.buffer, batch_size)`\n",
        "        # returns a list of tuples `(state, action, reward, next_state, done)`\n",
        "        # where:\n",
        "        # - `state`  and `next_state` are numpy arrays\n",
        "        # - `action` and `reward` are floats\n",
        "        # - `done` is a boolean\n",
        "        #\n",
        "        # `states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))`\n",
        "        # generates 5 tuples `state`, `action`, `reward`, `next_state` and `done`, each having `batch_size` elements.\n",
        "        states, actions, rewards, next_states, dones = zip(*random.sample(self.buffer, batch_size))\n",
        "        return np.array(states), actions, rewards, np.array(next_states), dones\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Return the current size of the buffer.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        int\n",
        "            The current size of the buffer.\n",
        "        \"\"\"\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fe0Hh-9v6Z1H"
      },
      "outputs": [],
      "source": [
        "def test_q_network_agent(\n",
        "    env: gym.Env, q_network: torch.nn.Module, num_episode: int = 1\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Test a naive agent in the given environment using the provided Q-network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env : gym.Env\n",
        "        The environment in which to test the agent.\n",
        "    q_network : torch.nn.Module\n",
        "        The Q-network to use for decision making.\n",
        "    num_episode : int, optional\n",
        "        The number of episodes to run, by default 1.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[float]\n",
        "        A list of rewards per episode.\n",
        "    \"\"\"\n",
        "    episode_reward_list = []\n",
        "\n",
        "    for episode_id in range(num_episode):\n",
        "        state, info = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "\n",
        "        while not done:\n",
        "            # Convert the state to a PyTorch tensor and add a batch dimension (unsqueeze)\n",
        "            state_tensor = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "            q_values = q_network(state_tensor)\n",
        "\n",
        "            action = torch.argmax(q_values).item()\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            episode_reward += float(reward)\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        episode_reward_list.append(episode_reward)\n",
        "        # print(f\"Episode reward: {episode_reward}\")\n",
        "\n",
        "    return episode_reward_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6tI16fpL6Z1H"
      },
      "outputs": [],
      "source": [
        "q_network = LinearNet(LL_observation_dim, LL_action_number, hidden_dim=128, n_layer = 3).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qIKEWUlJ6Z1H"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "VIDEO_PREFIX_DQN_NAIVE_UNTRAINED = \"_naive_untained\"\n",
        "\n",
        "NUM_EPISODES = 3\n",
        "\n",
        "file_path_list = [\n",
        "    FIGS_DIR / f\"{VIDEO_PREFIX_DQN_NAIVE_UNTRAINED}-episode-{episode_index}.mp4\"\n",
        "    for episode_index in range(NUM_EPISODES)\n",
        "]\n",
        "\n",
        "for file_path in file_path_list:\n",
        "    file_path.unlink(missing_ok=True)\n",
        "\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env,\n",
        "    video_folder=str(FIGS_DIR),\n",
        "    name_prefix=VIDEO_PREFIX_DQN_NAIVE_UNTRAINED,\n",
        "    episode_trigger=lambda x: True,\n",
        ")\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
        "\n",
        "test_q_network_agent(env, q_network, num_episode=NUM_EPISODES)\n",
        "\n",
        "# print(f'Episode time taken: {env.time_queue}')\n",
        "# print(f'Episode total rewards: {env.return_queue}')\n",
        "# print(f'Episode lengths: {env.length_queue}')\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
        "\n",
        "interact(video_selector, file_path=file_path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eITLctVm6Z1H"
      },
      "outputs": [],
      "source": [
        "def train_dqn2_agent(\n",
        "    env: gym.Env,\n",
        "    q_network: torch.nn.Module,\n",
        "    target_q_network: torch.nn.Module,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    loss_fn: Callable,\n",
        "    epsilon_greedy: EpsilonGreedy,\n",
        "    device: torch.device,\n",
        "    lr_scheduler: torch.optim.lr_scheduler.LRScheduler,\n",
        "    num_episodes: int,\n",
        "    gamma: float,\n",
        "    batch_size: int,\n",
        "    replay_buffer: ReplayBuffer,\n",
        "    target_q_network_sync_period: int,\n",
        ") -> List[float]:\n",
        "    \"\"\"\n",
        "    Train the Q-network on the given environment.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env : gym.Env\n",
        "        The environment to train on.\n",
        "    q_network : torch.nn.Module\n",
        "        The Q-network to train.\n",
        "    target_q_network : torch.nn.Module\n",
        "        The target Q-network to use for estimating the target Q-values.\n",
        "    optimizer : torch.optim.Optimizer\n",
        "        The optimizer to use for training.\n",
        "    loss_fn : callable\n",
        "        The loss function to use for training.\n",
        "    epsilon_greedy : EpsilonGreedy\n",
        "        The epsilon-greedy policy to use for action selection.\n",
        "    device : torch.device\n",
        "        The device to use for PyTorch computations.\n",
        "    lr_scheduler : torch.optim.lr_scheduler.LRScheduler\n",
        "        The learning rate scheduler to adjust the learning rate during training.\n",
        "    num_episodes : int\n",
        "        The number of episodes to train for.\n",
        "    gamma : float\n",
        "        The discount factor for future rewards.\n",
        "    batch_size : int\n",
        "        The size of the batch to use for training.\n",
        "    replay_buffer : ReplayBuffer\n",
        "        The replay buffer storing the experiences with their priorities.\n",
        "    target_q_network_sync_period : int\n",
        "        The number of episodes after which the target Q-network should be updated with the weights of the Q-network.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[float]\n",
        "        A list of cumulated rewards per episode.\n",
        "    \"\"\"\n",
        "    iteration = 0\n",
        "    episode_reward_list = []\n",
        "\n",
        "    for episode_index in tqdm(range(1, num_episodes)):\n",
        "        state, info = env.reset()\n",
        "        episode_reward = 0.0\n",
        "\n",
        "        for t in itertools.count():\n",
        "            # Get action, next_state and reward\n",
        "\n",
        "            action = epsilon_greedy(state)\n",
        "\n",
        "            next_state, reward, terminated, truncated, info = env.step(action)\n",
        "            done = terminated or truncated\n",
        "\n",
        "            replay_buffer.add(state, action, float(reward), next_state, done)\n",
        "\n",
        "            episode_reward += float(reward)\n",
        "\n",
        "            # Update the q_network weights with a batch of experiences from the buffer\n",
        "\n",
        "            if len(replay_buffer) > batch_size:\n",
        "                batch_states, batch_actions, batch_rewards, batch_next_states, batch_dones = replay_buffer.sample(batch_size)\n",
        "\n",
        "                # Convert to PyTorch tensors\n",
        "                batch_states_tensor = torch.tensor(batch_states, dtype=torch.float32, device=device)\n",
        "                batch_actions_tensor = torch.tensor(batch_actions, dtype=torch.long, device=device)\n",
        "                batch_rewards_tensor = torch.tensor(batch_rewards, dtype=torch.float32, device=device)\n",
        "                batch_next_states_tensor = torch.tensor(batch_next_states, dtype=torch.float32, device=device)\n",
        "                batch_dones_tensor = torch.tensor(batch_dones, dtype=torch.float32, device=device)\n",
        "\n",
        "                # Compute the target Q values for the batch\n",
        "                with torch.no_grad():\n",
        "                    next_state_q_values = target_q_network(batch_next_states_tensor)\n",
        "                    next_state_max_q_values, _ = next_state_q_values.max(1)\n",
        "\n",
        "                    targets = batch_rewards_tensor + (gamma * next_state_max_q_values * (1 - batch_dones_tensor))\n",
        "\n",
        "                current_q_values = q_network(batch_states_tensor).gather(1, batch_actions_tensor.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "                # Compute loss\n",
        "                loss = loss_fn(current_q_values, targets)\n",
        "\n",
        "                # Optimize the model\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                lr_scheduler.step()\n",
        "\n",
        "            # Update the target q-network weights\n",
        "            # Every episodes (e.g., every `target_q_network_sync_period` episodes), the weights of the target network are updated with the weights of the Q-network\n",
        "            iteration += 1\n",
        "            if episode_index % target_q_network_sync_period == 0:\n",
        "              target_q_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            state = next_state\n",
        "\n",
        "        episode_reward_list.append(episode_reward)\n",
        "        epsilon_greedy.decay_epsilon()\n",
        "\n",
        "    return episode_reward_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXpb5cPD6Z1I"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
        "dqn2_trains_result_list: List[List[Union[int, float]]] = [[], [], []]\n",
        "\n",
        "for train_index in range(NUMBER_OF_TRAININGS):\n",
        "    # Instantiate required objects\n",
        "\n",
        "    q_network = LinearNet(LL_observation_dim, LL_action_number, hidden_dim=128, n_layer = 3).to(device)\n",
        "\n",
        "    target_q_network = LinearNet(LL_observation_dim, LL_action_number, hidden_dim=128, n_layer = 3).to(device) # The target Q-network is used to compute the target Q-values for the loss function\n",
        "\n",
        "    # Initialize the target Q-network with the same weights as the Q-network (c.f. the \"Practical tips\" section of the exercise)\n",
        "    target_q_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=0.004, amsgrad=True)\n",
        "    # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
        "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=0.97, min_lr=0.0001)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    epsilon_greedy = EpsilonGreedy(\n",
        "        epsilon_start=0.85,\n",
        "        epsilon_min=0.010,\n",
        "        epsilon_decay=0.9675,\n",
        "        env=env,\n",
        "        q_network=q_network,\n",
        "    )\n",
        "\n",
        "    replay_buffer = ReplayBuffer(3000)\n",
        "\n",
        "    # Train the q-network\n",
        "\n",
        "    episode_reward_list = train_dqn2_agent(\n",
        "        env,\n",
        "        q_network,\n",
        "        target_q_network,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        epsilon_greedy,\n",
        "        device,\n",
        "        lr_scheduler,\n",
        "        num_episodes=200,\n",
        "        gamma=0.9,\n",
        "        batch_size=128,\n",
        "        replay_buffer=replay_buffer,\n",
        "        target_q_network_sync_period=12,\n",
        "    )\n",
        "    print(np.mean(episode_reward_list))\n",
        "    dqn2_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    dqn2_trains_result_list[1].extend(episode_reward_list)\n",
        "    dqn2_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "dqn2_trains_result_df = pd.DataFrame(\n",
        "    np.array(dqn2_trains_result_list).T,\n",
        "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
        ")\n",
        "dqn2_trains_result_df[\"agent\"] = \"DQN v2\"\n",
        "\n",
        "# Save the action-value estimation function\n",
        "\n",
        "torch.save(q_network, MODELS_DIR / \"project_dqn2_q_network.pth\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4zuSj-bq6Z1I"
      },
      "outputs": [],
      "source": [
        "dqn2_trains_result_df.to_csv(\"./data/project_dqn2_train_result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ttOj2ibM6Z1I"
      },
      "outputs": [],
      "source": [
        "print(len(replay_buffer))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Z_8VxjE6Z1I"
      },
      "outputs": [],
      "source": [
        "g = sns.relplot(\n",
        "    x=\"num_episodes\",\n",
        "    y=\"mean_final_episode_reward\",\n",
        "    kind=\"line\",\n",
        "    hue=\"agent\",\n",
        "    estimator=None,\n",
        "    units=\"training_index\",\n",
        "    data=dqn2_trains_result_df,\n",
        "    height=7,\n",
        "    aspect=2,\n",
        "    alpha=0.5,\n",
        ")\n",
        "plt.savefig(PLOTS_DIR / \"project_dqn2_trains_result.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6lHGbr16Z1I"
      },
      "outputs": [],
      "source": [
        "VIDEO_PREFIX_EX4_DQN2_TRAINED = \"project_dqn2_tained\"\n",
        "\n",
        "NUM_EPISODES = 3\n",
        "\n",
        "file_path_list = [\n",
        "    FIGS_DIR / f\"{VIDEO_PREFIX_EX4_DQN2_TRAINED}-episode-{episode_index}.mp4\"\n",
        "    for episode_index in range(NUM_EPISODES)\n",
        "]\n",
        "\n",
        "for file_path in file_path_list:\n",
        "    file_path.unlink(missing_ok=True)\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env,\n",
        "    video_folder=str(FIGS_DIR),\n",
        "    name_prefix=VIDEO_PREFIX_EX4_DQN2_TRAINED,\n",
        "    episode_trigger=lambda x: True,\n",
        ")\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
        "\n",
        "test_reward_list = test_q_network_agent(env, q_network, num_episode=NUM_EPISODES)\n",
        "\n",
        "# print(f'Episode time taken: {env.time_queue}')\n",
        "# print(f'Episode total rewards: {env.return_queue}')\n",
        "# print(f'Episode lengths: {env.length_queue}')\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
        "\n",
        "interact(video_selector, file_path=file_path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRZT5Wzo6Z1I"
      },
      "outputs": [],
      "source": [
        "mean_score_dqn2 = dqn2_trains_result_df[\"mean_final_episode_reward\"].mean()\n",
        "mean_score_dqn2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmnumonS6Z1I"
      },
      "outputs": [],
      "source": [
        "score_dqn2 = dqn2_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
        "score_dqn2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "29MC_44R6Z1K"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "dqn_reward_list = test_q_network_agent(env, q_network, num_episode=200)\n",
        "dqn_reward_index = np.arange(200)\n",
        "env.close()\n",
        "print(np.mean(dqn_reward_list), np.max(dqn_reward_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a8S5PFLS6Z1K"
      },
      "outputs": [],
      "source": [
        "g = sns.relplot(\n",
        "    x= dqn_reward_index,\n",
        "    y= dqn_reward_list,\n",
        "    kind=\"line\",\n",
        "    height=7,\n",
        "    aspect=2,\n",
        "    alpha=0.5,\n",
        ")\n",
        "plt.savefig(PLOTS_DIR / \"project_random_baseline.png\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_ofrbJr26Z1K"
      },
      "source": [
        "### Hyperparameter finetuning code for DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2J-cHhk66Z1K"
      },
      "outputs": [],
      "source": [
        "# Objective function for Optuna\n",
        "def objective_dqn(trial):\n",
        "    # Hyperparameters to be tuned\n",
        "    hidden_size = trial.suggest_int('hidden_size', 32, 256, log = True)\n",
        "    layer_num = trial.suggest_int('layer_num', 2,4)\n",
        "    lr_start = trial.suggest_loguniform('lr_start', 1e-3, 1e-2)\n",
        "    lr_min = trial.suggest_loguniform('lr_min', 1e-5, 1e-4)\n",
        "    lr_decay = trial.suggest_float('lr_decay', 0.95, 0.99)\n",
        "    epsilon_start = trial.suggest_float('epsilon_start', 0.8, 0.99)\n",
        "    epsilon_min = trial.suggest_float('epsilon_min', 0.005, 0.015)\n",
        "    epsilon_decay = trial.suggest_float('epsilon_decay', 0.95, 0.99)\n",
        "    len_buffer = trial.suggest_int('len_buffer', 2000, 10000, step = 500)\n",
        "    target_update_period = trial.suggest_int('target_update_period', 6, 20, step = 2)\n",
        "\n",
        "    env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "    q_network = LinearNet(LL_observation_dim, LL_action_number, hidden_dim= hidden_size, n_layer = layer_num).to(device)\n",
        "\n",
        "    target_q_network = LinearNet(LL_observation_dim, LL_action_number, hidden_dim=hidden_size, n_layer = layer_num).to(device) # The target Q-network is used to compute the target Q-values for the loss function\n",
        "\n",
        "    # Initialize the target Q-network with the same weights as the Q-network (c.f. the \"Practical tips\" section of the exercise)\n",
        "    target_q_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=lr_start, amsgrad=True)\n",
        "    # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
        "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=lr_decay, min_lr=lr_min)\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    epsilon_greedy = EpsilonGreedy(\n",
        "        epsilon_start=epsilon_start,\n",
        "        epsilon_min=epsilon_min,\n",
        "        epsilon_decay=epsilon_decay,\n",
        "        env=env,\n",
        "        q_network=q_network,\n",
        "    )\n",
        "\n",
        "    replay_buffer = ReplayBuffer(len_buffer)\n",
        "\n",
        "    # Train the q-network\n",
        "\n",
        "    episode_reward_list = train_dqn2_agent(\n",
        "        env,\n",
        "        q_network,\n",
        "        target_q_network,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        epsilon_greedy,\n",
        "        device,\n",
        "        lr_scheduler,\n",
        "        num_episodes=200,\n",
        "        gamma=0.9,\n",
        "        batch_size=128,\n",
        "        replay_buffer=replay_buffer,\n",
        "        target_q_network_sync_period=target_update_period,\n",
        "    )\n",
        "\n",
        "\n",
        "    mean_reward = np.mean(episode_reward_list)\n",
        "    std_reward = np.std(episode_reward_list)\n",
        "    min_reward = np.min(episode_reward_list)\n",
        "\n",
        "    lambda_std = 0.4\n",
        "    lambda_min = 0.2\n",
        "\n",
        "    objective_value = -mean_reward + lambda_std * std_reward - lambda_min * min_reward\n",
        "\n",
        "    return objective_value\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QhaF9fvX6Z1K"
      },
      "outputs": [],
      "source": [
        "# Optimize hyperparameters\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective_dqn, n_trials=50)\n",
        "best_hyperparams = study.best_params\n",
        "best_objective_value = study.best_value\n",
        "best_trial = study.best_trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AOldefdZ6Z1L"
      },
      "source": [
        "### Policy gradient"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8XmYgVqP6Z1L"
      },
      "outputs": [],
      "source": [
        "class PolicyNetwork(torch.nn.Module):\n",
        "    def __init__(self, dim_observation: int, n_action: int, hidden_dim: int, n_layer: int):\n",
        "        super().__init__()\n",
        "        self.dim_observation = dim_observation\n",
        "        self.n_action = n_action\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layer = n_layer\n",
        "        assert n_layer > 0\n",
        "        if n_layer == 1:\n",
        "            self.layer = torch.nn.Linear(dim_observation, n_action)\n",
        "        else:\n",
        "            layers = [torch.nn.Linear(dim_observation,hidden_dim),torch.nn.ReLU()]\n",
        "            for i in range(n_layer-2):\n",
        "                layers.append(torch.nn.Linear(hidden_dim,hidden_dim))\n",
        "                layers.append(torch.nn.ReLU())\n",
        "            layers.append(torch.nn.Linear(hidden_dim, n_action))\n",
        "            self.layer = torch.nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, state_tensor: torch.Tensor) -> torch.Tensor:\n",
        "        action_tensor = self.layer(state_tensor)\n",
        "        out = F.softmax(action_tensor)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iowQ9LQr6Z1L"
      },
      "outputs": [],
      "source": [
        "def sample_discrete_action(\n",
        "    policy_nn: PolicyNetwork, state: np.ndarray\n",
        ") -> Tuple[int, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Sample a discrete action based on the given state and policy network.\n",
        "\n",
        "    This function takes a state and a policy network, and returns a sampled action and its log probability.\n",
        "    The action is sampled from a categorical distribution defined by the output of the policy network.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    policy_nn : PolicyNetwork\n",
        "        The policy network that defines the probability distribution of the actions.\n",
        "    state : np.ndarray\n",
        "        The state based on which an action needs to be sampled.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[int, torch.Tensor]\n",
        "        The sampled action and its log probability.\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    # Convert the state into a tensor, specify its data type as float32, and send it to the device (CPU or GPU).\n",
        "    # The unsqueeze(0) function is used to add an extra dimension to the tensor to match the input shape required by the policy network.\n",
        "    state_tensor = torch.tensor(state,dtype = torch.float32, device = device).unsqueeze(0)\n",
        "\n",
        "    # Pass the state tensor through the policy network to get the parameters of the action probability distribution.\n",
        "    actions_probability_distribution_params = policy_nn(state_tensor)\n",
        "\n",
        "    # Create the categorical distribution used to sample an action from the parameters obtained from the policy network.\n",
        "    # See https://pytorch.org/docs/stable/distributions.html#categorical\n",
        "    actions_probability_distribution = torch.distributions.categorical.Categorical(probs=actions_probability_distribution_params)\n",
        "\n",
        "    # Sample an action from the categorical distribution.\n",
        "    sampled_action_tensor = actions_probability_distribution.sample()\n",
        "\n",
        "    # Convert the tensor containing the sampled action into a Python integer.\n",
        "    sampled_action = sampled_action_tensor.item()\n",
        "\n",
        "    # Calculate the log probability of the sampled action according to the categorical distribution.\n",
        "    sampled_action_log_probability = actions_probability_distribution.log_prob(sampled_action_tensor)\n",
        "\n",
        "    # Return the sampled action and its log probability.\n",
        "    return sampled_action, sampled_action_log_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DozvHiA26Z1L"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "\n",
        "policy_nn = PolicyNetwork(LL_observation_dim, LL_action_number).to(device)\n",
        "\n",
        "state, info = env.reset()\n",
        "theta = list(policy_nn.parameters())\n",
        "action, action_log_probability = sample_discrete_action(policy_nn, state)\n",
        "\n",
        "print(\"state:\", state)\n",
        "print(\"theta:\", theta)\n",
        "print(\"sampled action:\", action)\n",
        "print(\"log probability of the sampled action:\", action_log_probability)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yv9Xfkn86Z1L"
      },
      "outputs": [],
      "source": [
        "def sample_one_episode(\n",
        "    env: gym.Env, policy_nn: PolicyNetwork, max_episode_duration: int\n",
        ") -> Tuple[List[np.ndarray], List[int], List[float], List[torch.Tensor]]:\n",
        "    \"\"\"\n",
        "    Execute one episode within the `env` environment utilizing the policy defined by the `policy_nn` parameter.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env : gym.Env\n",
        "        The environment to play in.\n",
        "    policy_nn : PolicyNetwork\n",
        "        The policy neural network.\n",
        "    max_episode_duration : int\n",
        "        The maximum duration of the episode.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[List[np.ndarray], List[int], List[float], List[torch.Tensor]]\n",
        "        The states, actions, rewards, and log probability of action for each time step in the episode.\n",
        "    \"\"\"\n",
        "    state_t, info = env.reset()\n",
        "\n",
        "    episode_states = []\n",
        "    episode_actions = []\n",
        "    episode_log_prob_actions = []\n",
        "    episode_rewards = []\n",
        "    episode_states.append(state_t)\n",
        "\n",
        "    for t in range(max_episode_duration):\n",
        "\n",
        "        # Sample a discrete action and its log probability from the policy network based on the current state\n",
        "        action_t,log_prob_action_t = sample_discrete_action(policy_nn, state_t)\n",
        "\n",
        "        # Execute the sampled action in the environment, which returns the new state, reward, and whether the episode has terminated or been truncated\n",
        "        state_t, reward_t,terminated, truncated, info = env.step(action_t)\n",
        "\n",
        "        # Check if the episode is done, either due to termination (reaching a terminal state) or truncation (reaching a maximum number of steps)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        # Append the new state, action, action log probability and reward to their respective lists\n",
        "\n",
        "        episode_states.append(state_t)\n",
        "        episode_actions.append(action_t)\n",
        "        episode_log_prob_actions.append(log_prob_action_t)\n",
        "        episode_rewards.append(float(reward_t))\n",
        "\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return episode_states, episode_actions, episode_rewards, episode_log_prob_actions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kMeh3Lmh6Z1L"
      },
      "outputs": [],
      "source": [
        "VIDEO_PREFIX_REINFORCE_UNTRAINED = \"project_reinforce_untained\"\n",
        "\n",
        "NUM_EPISODES = 3\n",
        "\n",
        "file_path_list = [\n",
        "    FIGS_DIR / f\"{VIDEO_PREFIX_REINFORCE_UNTRAINED}-episode-{episode_index}.mp4\"\n",
        "    for episode_index in range(NUM_EPISODES)\n",
        "]\n",
        "\n",
        "for file_path in file_path_list:\n",
        "    file_path.unlink(missing_ok=True)\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env,\n",
        "    video_folder=str(FIGS_DIR),\n",
        "    name_prefix=VIDEO_PREFIX_REINFORCE_UNTRAINED,\n",
        "    episode_trigger=lambda x: True,\n",
        ")\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
        "\n",
        "for episode_index in range(NUM_EPISODES):\n",
        "    policy_nn = PolicyNetwork(env.observation_space._shape[0], env.action_space.n).to(device)\n",
        "    episode_states, episode_actions, episode_rewards, episode_log_prob_actions = sample_one_episode(env,policy_nn,max_episode_duration = 500)\n",
        "\n",
        "print(f\"Episode time taken: {env.time_queue}\")\n",
        "print(f\"Episode total rewards: {env.return_queue}\")\n",
        "print(f\"Episode lengths: {env.length_queue}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
        "\n",
        "interact(video_selector, file_path=file_path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQ9cVhyl6Z1L"
      },
      "outputs": [],
      "source": [
        "def avg_return_on_multiple_episodes(\n",
        "    env: gym.Env,\n",
        "    policy_nn: PolicyNetwork,\n",
        "    num_test_episode: int,\n",
        "    max_episode_duration: int,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Play multiple episodes of the environment and calculate the average return.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env : gym.Env\n",
        "        The environment to play in.\n",
        "    policy_nn : PolicyNetwork\n",
        "        The policy neural network.\n",
        "    num_test_episode : int\n",
        "        The number of episodes to play.\n",
        "    max_episode_duration : int\n",
        "        The maximum duration of an episode.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    float\n",
        "        The average return.\n",
        "    \"\"\"\n",
        "\n",
        "    # TODO...\n",
        "    average_return = 0.0\n",
        "    for episode_index in range(num_test_episode):\n",
        "        episode_states, episode_actions, episode_rewards, episode_log_prob_actions = sample_one_episode(env,policy_nn,max_episode_duration)\n",
        "        average_return += sum(episode_rewards)\n",
        "    average_return /= num_test_episode\n",
        "    return average_return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WJrwVib6Z1L"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "\n",
        "policy_nn = PolicyNetwork(env.observation_space._shape[0], env.action_space.n).to(device)\n",
        "average_return = avg_return_on_multiple_episodes(env,policy_nn,20,500)\n",
        "\n",
        "print(average_return)\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kRtlI0Ri6Z1L"
      },
      "outputs": [],
      "source": [
        "def train_reinforce_discrete(\n",
        "    env: gym.Env,\n",
        "    num_train_episodes: int,\n",
        "    num_test_per_episode: int,\n",
        "    max_episode_duration: int,\n",
        "    learning_rate: float,\n",
        ") -> Tuple[PolicyNetwork, List[float]]:\n",
        "    \"\"\"\n",
        "    Train a policy using the REINFORCE algorithm.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    env : gym.Env\n",
        "        The environment to train in.\n",
        "    num_train_episodes : int\n",
        "        The number of training episodes.\n",
        "    num_test_per_episode : int\n",
        "        The number of tests to perform per episode.\n",
        "    max_episode_duration : int\n",
        "        The maximum length of an episode, by default EPISODE_DURATION.\n",
        "    learning_rate : float\n",
        "        The initial step size.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    Tuple[PolicyNetwork, List[float]]\n",
        "        The final trained policy and the average returns for each episode.\n",
        "    \"\"\"\n",
        "    episode_avg_return_list = []\n",
        "\n",
        "    policy_nn = PolicyNetwork(env.observation_space._shape[0], env.action_space.n, 128, 3).to(device)\n",
        "    optimizer = torch.optim.Adam(policy_nn.parameters(), lr=learning_rate)\n",
        "\n",
        "    for episode_index in tqdm(range(num_train_episodes)):\n",
        "        episode_states, episode_actions, episode_rewards, episode_log_prob_actions = sample_one_episode(env,policy_nn,max_episode_duration)\n",
        "        # T = len(episode_states) - 1\n",
        "        # returns = torch.tensor(episode_rewards, dtype=torch.float32, device=device)\n",
        "        # returns = returns.flip(dims=(0,)).cumsum(dim=0).flip(dims=(0,))\n",
        "        # for i in range(T):\n",
        "        #     loss = -returns[i]*episode_log_prob_actions[i]\n",
        "        #     optimizer.zero_grad()\n",
        "        #     loss.backward()\n",
        "        #     optimizer.step()\n",
        "        episode_returns = []\n",
        "        R = 0\n",
        "        for r in reversed(episode_rewards):\n",
        "            R = r + R\n",
        "            episode_returns.insert(0, R)\n",
        "        episode_returns = torch.tensor(episode_returns, dtype=torch.float32, device=device)\n",
        "\n",
        "        loss = 0\n",
        "        for log_prob, R in zip(episode_log_prob_actions, episode_returns):\n",
        "            loss -= log_prob * R\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "\n",
        "        # Test the current policy\n",
        "        test_avg_return = avg_return_on_multiple_episodes(\n",
        "            env=env,\n",
        "            policy_nn=policy_nn,\n",
        "            num_test_episode=num_test_per_episode,\n",
        "            max_episode_duration=max_episode_duration,\n",
        "        )\n",
        "\n",
        "        # Monitoring\n",
        "        episode_avg_return_list.append(test_avg_return)\n",
        "\n",
        "    return policy_nn, episode_avg_return_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ9rY2wZ6Z1L"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "\n",
        "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
        "reinforce_trains_result_list: List[List[Union[int, float]]] = [[], [], []]\n",
        "\n",
        "for train_index in range(NUMBER_OF_TRAININGS):\n",
        "    # Train the agent\n",
        "    reinforce_policy_nn, episode_reward_list = train_reinforce_discrete(\n",
        "        env=env,\n",
        "        num_train_episodes=200,\n",
        "        num_test_per_episode=5,\n",
        "        max_episode_duration=500,\n",
        "        learning_rate=0.001,\n",
        "    )\n",
        "\n",
        "    reinforce_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    reinforce_trains_result_list[1].extend(episode_reward_list)\n",
        "    reinforce_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "reinforce_trains_result_df = pd.DataFrame(\n",
        "    np.array(reinforce_trains_result_list).T,\n",
        "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
        ")\n",
        "reinforce_trains_result_df[\"agent\"] = \"REINFORCE\"\n",
        "\n",
        "# Save the action-value estimation function of the last train\n",
        "\n",
        "torch.save(reinforce_policy_nn, MODELS_DIR / \"project_reinforce_policy_network.pth\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f697i-LR6Z1L"
      },
      "outputs": [],
      "source": [
        "reinforce_trains_result_df.to_csv(\"./data/project_reinforce_trains_result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cHP5IOfj6Z1L"
      },
      "outputs": [],
      "source": [
        "g = sns.relplot(\n",
        "    x=\"num_episodes\",\n",
        "    y=\"mean_final_episode_reward\",\n",
        "    kind=\"line\",\n",
        "    hue=\"agent\",\n",
        "    estimator=None,\n",
        "    units=\"training_index\",\n",
        "    data=reinforce_trains_result_df,\n",
        "    height=7,\n",
        "    aspect=2,\n",
        "    alpha=0.5,\n",
        ")\n",
        "plt.savefig(PLOTS_DIR / \"project_reinforce_trains_result.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l3S3tPGY6Z1L"
      },
      "outputs": [],
      "source": [
        "mean_score_reinforce = reinforce_trains_result_df[\"mean_final_episode_reward\"].mean()\n",
        "\n",
        "mean_score_reinforce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u4kGNDqW6Z1L"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "mean_test_reward = avg_return_on_multiple_episodes(env,reinforce_policy_nn,200,500)\n",
        "env.close()\n",
        "mean_test_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8Rs_UYsw6Z1L"
      },
      "outputs": [],
      "source": [
        "dqn2_trains_result_df = pd.read_csv(\"./data/project_dqn2_train_result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7aYE6ohz6Z1L"
      },
      "outputs": [],
      "source": [
        "all_trains_result_df = pd.concat(\n",
        "    [\n",
        "        dqn2_trains_result_df,\n",
        "        reinforce_trains_result_df,\n",
        "    ]\n",
        ")\n",
        "g = sns.relplot(\n",
        "    x=\"num_episodes\",\n",
        "    y=\"mean_final_episode_reward\",\n",
        "    kind=\"line\",\n",
        "    hue=\"agent\",\n",
        "    data=all_trains_result_df,\n",
        "    height=7,\n",
        "    aspect=2,\n",
        ")\n",
        "plt.savefig(PLOTS_DIR / \"project_reinforce_trains_result_agg.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzLxVnGD6Z1L"
      },
      "outputs": [],
      "source": [
        "VIDEO_PREFIX_PROJECT_REINFORCE_TRAINED = \"project_reinforce_tained\"\n",
        "\n",
        "NUM_EPISODES = 3\n",
        "\n",
        "file_path_list = [\n",
        "    FIGS_DIR / f\"{VIDEO_PREFIX_PROJECT_REINFORCE_TRAINED}-episode-{episode_index}.mp4\"\n",
        "    for episode_index in range(NUM_EPISODES)\n",
        "]\n",
        "\n",
        "for file_path in file_path_list:\n",
        "    file_path.unlink(missing_ok=True)\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env,\n",
        "    video_folder=str(FIGS_DIR),\n",
        "    name_prefix=VIDEO_PREFIX_PROJECT_REINFORCE_TRAINED,\n",
        "    episode_trigger=lambda x: True,\n",
        ")\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
        "\n",
        "max_episode_duration = 500\n",
        "for episode_index in range(NUM_EPISODES):\n",
        "    state_t, info = env.reset()\n",
        "    episode_states = []\n",
        "    episode_actions = []\n",
        "    episode_rewards = []\n",
        "    episode_states.append(state_t)\n",
        "\n",
        "    for t in range(max_episode_duration):\n",
        "        state_tensor_t = torch.tensor(state_t,dtype = torch.float32, device = device).unsqueeze(0)\n",
        "        actions_probability_distribution_params = reinforce_policy_nn(state_tensor_t)\n",
        "        action_t = torch.argmax(actions_probability_distribution_params).item()\n",
        "        state_t, reward_t,terminated, truncated, info = env.step(action_t)\n",
        "        done = terminated or truncated\n",
        "        episode_states.append(state_t)\n",
        "        episode_actions.append(action_t)\n",
        "        episode_rewards.append(float(reward_t))\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "print(f\"Episode time taken: {env.time_queue}\")\n",
        "print(f\"Episode total rewards: {env.return_queue}\")\n",
        "print(f\"Episode lengths: {env.length_queue}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
        "\n",
        "interact(video_selector, file_path=file_path_list)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OP3LyjNp6Z1L"
      },
      "source": [
        "### Hyperparameters Finetuning Code for REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zvVV0cVM6Z1L"
      },
      "outputs": [],
      "source": [
        "def new_train_reinforce_discrete(\n",
        "    env: gym.Env,\n",
        "    hidden_dim: int,\n",
        "    n_layers: int,\n",
        "    num_train_episodes: int,\n",
        "    num_test_per_episode: int,\n",
        "    max_episode_duration: int,\n",
        "    lr_start = 0.01,\n",
        "    lr_decay = 0.97,\n",
        "    min_lr = 0.0001,\n",
        ") -> Tuple[PolicyNetwork, List[float]]:\n",
        "    ### Add the same learning rate scheduler as DQN for REINFORCE\n",
        "    episode_avg_return_list = []\n",
        "\n",
        "    policy_nn = PolicyNetwork(env.observation_space._shape[0], env.action_space.n, hidden_dim, n_layers).to(device)\n",
        "    optimizer = torch.optim.Adam(policy_nn.parameters(), lr=lr_start)\n",
        "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=lr_decay, min_lr=min_lr)\n",
        "\n",
        "    for episode_index in tqdm(range(num_train_episodes)):\n",
        "        episode_states, episode_actions, episode_rewards, episode_log_prob_actions = sample_one_episode(env,policy_nn,max_episode_duration)\n",
        "\n",
        "        episode_returns = []\n",
        "        R = 0\n",
        "        for r in reversed(episode_rewards):\n",
        "            R = r + R\n",
        "            episode_returns.insert(0, R)\n",
        "        episode_returns = torch.tensor(episode_returns, dtype=torch.float32, device=device)\n",
        "\n",
        "        loss = 0\n",
        "        for log_prob, R in zip(episode_log_prob_actions, episode_returns):\n",
        "            loss -= log_prob * R\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        lr_scheduler.step()\n",
        "\n",
        "        # Test the current policy\n",
        "        test_avg_return = avg_return_on_multiple_episodes(\n",
        "            env=env,\n",
        "            policy_nn=policy_nn,\n",
        "            num_test_episode=num_test_per_episode,\n",
        "            max_episode_duration=max_episode_duration,\n",
        "        )\n",
        "\n",
        "        # Monitoring\n",
        "        episode_avg_return_list.append(test_avg_return)\n",
        "\n",
        "    return policy_nn, episode_avg_return_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uguPadvw6Z1L"
      },
      "outputs": [],
      "source": [
        "def objective_pg(trial):\n",
        "    # Hyperparameters to be tuned\n",
        "    hidden_size = trial.suggest_int('hidden_size', 32, 256, log = True)\n",
        "    layer_num = trial.suggest_int('layer_num', 2,4)\n",
        "    lr_start = trial.suggest_loguniform('lr_start', 1e-3, 1e-2)\n",
        "    lr_min = trial.suggest_loguniform('lr_min', 1e-5, 1e-4)\n",
        "    lr_decay = trial.suggest_float('lr_decay', 0.95, 0.99)\n",
        "\n",
        "\n",
        "    env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "\n",
        "\n",
        "\n",
        "    reinforce_policy_nn, episode_reward_list = new_train_reinforce_discrete(\n",
        "        env=env,\n",
        "        hidden_dim = hidden_size,\n",
        "        n_layers = layer_num,\n",
        "        num_train_episodes=200,\n",
        "        num_test_per_episode=5,\n",
        "        max_episode_duration=500,\n",
        "        lr_start = lr_start,\n",
        "        lr_decay = lr_decay,\n",
        "        min_lr = lr_min\n",
        "    )\n",
        "\n",
        "    mean_reward = np.mean(episode_reward_list)\n",
        "    std_reward = np.std(episode_reward_list)\n",
        "    min_reward = np.min(episode_reward_list)\n",
        "\n",
        "    lambda_std = 0.4\n",
        "    lambda_min = 0.2\n",
        "\n",
        "    objective_value = -mean_reward + lambda_std * std_reward - lambda_min * min_reward\n",
        "\n",
        "    return objective_value"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MJYUNeYo6Z1L"
      },
      "outputs": [],
      "source": [
        "# Optimize hyperparameters\n",
        "study = optuna.create_study(direction='minimize')\n",
        "study.optimize(objective_pg, n_trials=100)\n",
        "best_hyperparams = study.best_params\n",
        "best_objective_value = study.best_value\n",
        "best_trial = study.best_trial"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UrmAx2NM6Z1M"
      },
      "source": [
        "## Continuous Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C45PAIQ86Z1M"
      },
      "outputs": [],
      "source": [
        "!pip install swig\n",
        "!pip install box2d-py==2.3.8 --no-build-isolation\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "09kt-EIt6Z1M"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=True, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lf55cnZw6Z1M"
      },
      "source": [
        "### Proximal Policy Optimization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MRq4QTzm6Z1M"
      },
      "outputs": [],
      "source": [
        "class Actor(torch.nn.Module):\n",
        "    def __init__(self, state_dim, action_dim):\n",
        "        super().__init__()\n",
        "        self.fc = torch.nn.Sequential(torch.nn.Linear(state_dim, 128), torch.nn.ReLU(), torch.nn.Linear(128,128), torch.nn.ReLU())\n",
        "        self.mu = torch.nn.Linear(128, action_dim)\n",
        "        self.log_std = torch.nn.Linear(128, action_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.fc(state)\n",
        "        mu = torch.tanh(self.mu(x))\n",
        "        log_std = self.log_std(x)\n",
        "        std = torch.exp(log_std)\n",
        "        return mu, std\n",
        "\n",
        "    def get_action(self, state):\n",
        "        mu, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mu, std)\n",
        "        action = dist.rsample()\n",
        "        action = action.clamp(-1, 1)\n",
        "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
        "        # log_prob = torch.clamp(log_prob, min=-1e6, max=1e6)\n",
        "        return action, log_prob\n",
        "\n",
        "    def get_log_prob(self, state, action):\n",
        "        mean, std = self.forward(state)\n",
        "        dist = torch.distributions.Normal(mean, std)\n",
        "        log_prob = dist.log_prob(action).sum(dim = -1)\n",
        "        return log_prob\n",
        "\n",
        "\n",
        "\n",
        "class Critic(torch.nn.Module):\n",
        "    def __init__(self, state_dim):\n",
        "        super().__init__()\n",
        "        self.fc = torch.nn.Sequential(\n",
        "            torch.nn.Linear(state_dim, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 128),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(128, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, state):\n",
        "        return self.fc(state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOzJ-W4y6Z1M"
      },
      "outputs": [],
      "source": [
        "def evaluate_policy(actor, env, num_test_episodes=5, max_episode_duration = 500):\n",
        "    total_reward = 0.0\n",
        "    for _ in range(num_test_episodes):\n",
        "        state,info = env.reset()\n",
        "        done = False\n",
        "        episode_reward = 0.0\n",
        "\n",
        "        for i in range(max_episode_duration):\n",
        "            # print(state, state.shape)\n",
        "            state = torch.tensor(state, dtype=torch.float32) #.unsqueeze(0)\n",
        "            action, _ = actor.get_action(state)\n",
        "            next_state, reward, terminated, truncated, info = env.step(action.detach().numpy())  #.squeeze(0)\n",
        "            done = terminated or truncated\n",
        "            episode_reward += reward\n",
        "            state = next_state\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "        total_reward += episode_reward\n",
        "\n",
        "    avg_reward = total_reward / num_test_episodes\n",
        "    # print(f\"Average reward over {num_test_episodes} test episodes: {avg_reward}\")\n",
        "    return avg_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "68YDegYT6Z1M"
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, state, action, log_prob, reward, value):\n",
        "        self.buffer.append((state, action, log_prob, reward, value))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, replace=False)\n",
        "        states, actions, log_probs, rewards, values = zip(*[self.buffer[idx] for idx in indices])\n",
        "        return (\n",
        "            torch.stack(states),\n",
        "            torch.stack(actions),\n",
        "            torch.stack(log_probs),\n",
        "            torch.tensor(rewards, dtype=torch.float32),\n",
        "            torch.tensor(values, dtype=torch.float32),\n",
        "        )\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DF_QWKju6Z1M"
      },
      "outputs": [],
      "source": [
        "def compute_advantage(rewards, values, gamma=0.99, lam=0.95):\n",
        "    returns = []\n",
        "    adv = 0\n",
        "    G = 0\n",
        "    for i in reversed(range(len(rewards))):\n",
        "        delta = rewards[i] + gamma * (values[i + 1] if i + 1 < len(values) else 0) - values[i]\n",
        "        adv = delta + gamma * lam * adv\n",
        "        G = rewards[i] + gamma * G\n",
        "        returns.insert(0, G)\n",
        "\n",
        "    returns = torch.tensor(returns, dtype=torch.float32)\n",
        "    advantage = returns - values\n",
        "    return advantage, returns\n",
        "\n",
        "def sample_trajectory(env, actor, critic, max_steps=1000):\n",
        "    state = env.reset()[0]\n",
        "    log_probs, rewards, values, states, actions = [], [], [], [], []\n",
        "\n",
        "    for _ in range(max_steps):\n",
        "        state = torch.tensor(state, dtype=torch.float32)  #.unsqueeze(0)\n",
        "        value = critic(state).item()\n",
        "        action, log_prob = actor.get_action(state)\n",
        "\n",
        "        next_state, reward, terminated, truncated, info = env.step(action.detach().numpy())   #.squeeze(0)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        states.append(state)\n",
        "        actions.append(action)\n",
        "        log_probs.append(log_prob)\n",
        "        rewards.append(reward)\n",
        "        values.append(value)\n",
        "\n",
        "        state = next_state\n",
        "        if done:\n",
        "            break\n",
        "\n",
        "    return states, actions, log_probs, rewards, torch.tensor(values)\n",
        "\n",
        "# def train_PPO(env: gym.Env, num_train_episodes: int, num_test_per_episode: int, max_episode_duration: int, learning_rate: float):\n",
        "#     state_dim = env.observation_space.shape[0]\n",
        "#     action_dim = env.action_space.shape[0]\n",
        "\n",
        "#     actor = Actor(state_dim, action_dim)\n",
        "#     critic = Critic(state_dim)\n",
        "\n",
        "#     actor_optimizer = torch.optim.Adam(actor.parameters(), lr=learning_rate) #lr=0.0003\n",
        "#     critic_optimizer = torch.optim.Adam(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "#     epsilon = 0.2  # PPO clip parameter\n",
        "#     gamma = 0.9\n",
        "#     lam = 0.95  # GAE-Lambda\n",
        "#     # epochs = 10\n",
        "\n",
        "#     episode_reward_list = []\n",
        "\n",
        "#     for episode in tqdm(range(num_train_episodes)):\n",
        "\n",
        "#         states, actions, log_probs, rewards, values = sample_trajectory(env, actor, critic, max_episode_duration)\n",
        "#         advantage, returns = compute_advantage(rewards, values, gamma, lam)\n",
        "\n",
        "#         states = torch.stack(states)\n",
        "#         actions = torch.stack(actions)\n",
        "#         log_probs = torch.stack(log_probs)\n",
        "\n",
        "#         new_actions, new_log_probs = actor.get_action(states)\n",
        "#         ratio = torch.exp(new_log_probs - log_probs)\n",
        "\n",
        "\n",
        "#         surr1 = ratio * advantage\n",
        "#         surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantage\n",
        "#         loss_actor = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "#         loss_critic = (returns - critic(states)).pow(2).mean()\n",
        "\n",
        "#         actor_optimizer.zero_grad()\n",
        "#         loss_actor.backward()\n",
        "#         actor_optimizer.step()\n",
        "\n",
        "#         critic_optimizer.zero_grad()\n",
        "#         loss_critic.backward()\n",
        "#         critic_optimizer.step()\n",
        "\n",
        "#         test_reward = evaluate_policy(actor,env,num_test_per_episode)\n",
        "#         episode_reward_list.append(test_reward)\n",
        "\n",
        "#     return actor, critic, episode_reward_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7KYXgKw6Z1M"
      },
      "outputs": [],
      "source": [
        "# def train_PPO(env: gym.Env, num_train_episodes: int, num_test_per_episode: int,\n",
        "#               max_episode_duration: int, learning_rate: float):\n",
        "\n",
        "#     state_dim = env.observation_space.shape[0]\n",
        "#     action_dim = env.action_space.shape[0]\n",
        "\n",
        "#     actor = Actor(state_dim, action_dim)\n",
        "#     critic = Critic(state_dim)\n",
        "\n",
        "#     actor_optimizer = torch.optim.Adam(actor.parameters(), lr=learning_rate)\n",
        "#     critic_optimizer = torch.optim.Adam(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "#     epsilon = 0.2\n",
        "#     gamma = 0.9\n",
        "#     lam = 0.95\n",
        "\n",
        "#     episode_reward_list = []\n",
        "\n",
        "#     for episode in tqdm(range(num_train_episodes)):\n",
        "#         states, actions, log_probs_old, rewards, values = sample_trajectory(env, actor, critic, max_episode_duration)\n",
        "#         advantage, returns = compute_advantage(rewards, values, gamma, lam)\n",
        "\n",
        "#         states = torch.stack(states)\n",
        "#         actions = torch.stack(actions)\n",
        "#         # advantage = (advantage - advantage.mean()) / (advantage.std() + 1e-8)\n",
        "#         log_probs_old = torch.stack(log_probs_old).detach()\n",
        "\n",
        "\n",
        "#         for epoch in range(10):\n",
        "\n",
        "#             new_log_probs = actor.get_log_prob(states,actions)\n",
        "#             print(new_log_probs)\n",
        "#             ratio = torch.exp(new_log_probs - log_probs_old)\n",
        "#             surr1 = ratio * advantage\n",
        "#             surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantage\n",
        "#             loss_actor = -torch.min(surr1, surr2).mean()\n",
        "#             loss_critic = (returns - critic(states)).pow(2).mean()\n",
        "\n",
        "#             actor_optimizer.zero_grad()\n",
        "#             critic_optimizer.zero_grad()\n",
        "#             loss_actor.backward(retain_graph=True)\n",
        "#             loss_critic.backward(retain_graph=True)\n",
        "#             actor_optimizer.step()\n",
        "#             critic_optimizer.step()\n",
        "\n",
        "\n",
        "#         test_reward = evaluate_policy(actor, env, num_test_per_episode)\n",
        "#         episode_reward_list.append(test_reward)\n",
        "\n",
        "#     return actor, critic, episode_reward_list\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EJ7_OFUc7u1m"
      },
      "outputs": [],
      "source": [
        "def train_PPO(env: gym.Env, num_train_episodes: int, num_test_per_episode: int,\n",
        "              max_episode_duration: int, learning_rate: float):\n",
        "\n",
        "    state_dim = env.observation_space.shape[0]\n",
        "    action_dim = env.action_space.shape[0]\n",
        "\n",
        "    actor = Actor(state_dim, action_dim)\n",
        "    critic = Critic(state_dim)\n",
        "\n",
        "    actor_optimizer = torch.optim.Adam(actor.parameters(), lr=learning_rate)\n",
        "    critic_optimizer = torch.optim.Adam(critic.parameters(), lr=learning_rate)\n",
        "\n",
        "    epsilon = 0.2\n",
        "    gamma = 0.9\n",
        "    lam = 0.95\n",
        "    num_epochs = 3\n",
        "\n",
        "    episode_reward_list = []\n",
        "\n",
        "    for episode in tqdm(range(num_train_episodes)):\n",
        "        # Collect a trajectory first\n",
        "        with torch.no_grad():\n",
        "            states_raw, actions_raw, log_probs_raw, rewards, values_raw = sample_trajectory(env, actor, critic, max_episode_duration)\n",
        "            advantage, returns = compute_advantage(rewards, values_raw, gamma, lam)\n",
        "\n",
        "            # Convert everything to tensors\n",
        "            states = torch.stack(states_raw)\n",
        "            actions = torch.stack(actions_raw)\n",
        "            log_probs_old = torch.stack(log_probs_raw)\n",
        "            advantage = torch.tensor(advantage, dtype=torch.float32)\n",
        "            returns = torch.tensor(returns, dtype=torch.float32)\n",
        "\n",
        "        # Multi-epoch PPO update\n",
        "        for _ in range(num_epochs):\n",
        "            with torch.enable_grad():\n",
        "                mu, std = actor.forward(states)\n",
        "\n",
        "                mu = torch.tanh(mu) # stablize mu with tanh. ranges in [-1, 1] anyway\n",
        "                std = F.softplus(std) + 1e-5  # add a small constant to prevent std from being 0\n",
        "\n",
        "                dist = torch.distributions.Normal(mu, std)\n",
        "                log_probs = dist.log_prob(actions).sum(dim=-1)\n",
        "\n",
        "\n",
        "                ratio = torch.exp(log_probs - log_probs_old)\n",
        "                surr1 = ratio * advantage\n",
        "                surr2 = torch.clamp(ratio, 1 - epsilon, 1 + epsilon) * advantage\n",
        "                actor_loss = -torch.min(surr1, surr2).mean()\n",
        "\n",
        "                values = critic(states)\n",
        "                critic_loss = (returns - values).pow(2).mean()\n",
        "\n",
        "            actor_optimizer.zero_grad()\n",
        "            actor_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(actor.parameters(), 1.0)  # Clip gradients\n",
        "            actor_optimizer.step()\n",
        "\n",
        "            critic_optimizer.zero_grad()\n",
        "            critic_loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(critic.parameters(), 1.0)  # Clip gradients\n",
        "            critic_optimizer.step()\n",
        "\n",
        "        test_reward = evaluate_policy(actor, env, num_test_per_episode)\n",
        "        episode_reward_list.append(test_reward)\n",
        "\n",
        "    return actor, critic, episode_reward_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VAKkoHbl6Z1M"
      },
      "outputs": [],
      "source": [
        "VIDEO_PREFIX_PROJECT_PPO_UNTRAINED = \"project_PPO_untained\"\n",
        "\n",
        "NUM_EPISODES = 3\n",
        "\n",
        "file_path_list = [\n",
        "    FIGS_DIR / f\"{VIDEO_PREFIX_PROJECT_PPO_UNTRAINED}-episode-{episode_index}.mp4\"\n",
        "    for episode_index in range(NUM_EPISODES)\n",
        "]\n",
        "\n",
        "for file_path in file_path_list:\n",
        "    file_path.unlink(missing_ok=True)\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", continuous=True, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env,\n",
        "    video_folder=str(FIGS_DIR),\n",
        "    name_prefix=VIDEO_PREFIX_PROJECT_PPO_UNTRAINED,\n",
        "    episode_trigger=lambda x: True,\n",
        ")\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
        "\n",
        "for episode_index in range(NUM_EPISODES):\n",
        "    critic_nn = Critic(env.observation_space._shape[0])\n",
        "    actor_nn = Actor(env.observation_space._shape[0], env.action_space._shape[0]) # TODO...\n",
        "    episode_states, episode_actions, episode_log_prob_actions, episode_rewards, episode_values = sample_trajectory(env,actor_nn,critic_nn, max_steps = 1000) # TODO...\n",
        "\n",
        "print(f\"Episode time taken: {env.time_queue}\")\n",
        "print(f\"Episode total rewards: {env.return_queue}\")\n",
        "print(f\"Episode lengths: {env.length_queue}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
        "\n",
        "interact(video_selector, file_path=file_path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipBYtDno6Z1M"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=True, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "\n",
        "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS\n",
        "PPO_trains_result_list: List[List[Union[int, float]]] = [[], [], []]\n",
        "\n",
        "for train_index in range(NUMBER_OF_TRAININGS):\n",
        "    # Train the agent\n",
        "    actor_nn, critic_nn, episode_reward_list = train_PPO(\n",
        "        env=env,\n",
        "        num_train_episodes=200,\n",
        "        num_test_per_episode=5,\n",
        "        max_episode_duration=500,\n",
        "        learning_rate=0.0001,\n",
        "    )\n",
        "\n",
        "    PPO_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    PPO_trains_result_list[1].extend(episode_reward_list)\n",
        "    PPO_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "PPO_trains_result_df = pd.DataFrame(\n",
        "    np.array(PPO_trains_result_list).T,\n",
        "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
        ")\n",
        "PPO_trains_result_df[\"agent\"] = \"PPO\"\n",
        "\n",
        "# Save the action-value estimation function of the last train\n",
        "\n",
        "torch.save(actor_nn, MODELS_DIR / \"project_PPO_actor_network.pth\")\n",
        "torch.save(critic_nn, MODELS_DIR / \"project_PPO_critic_network.pth\")\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pdpQjyBW6Z1M"
      },
      "outputs": [],
      "source": [
        "VIDEO_PREFIX_PROJECT_PPO_TRAINED = \"project_PPO_trained\"\n",
        "\n",
        "NUM_EPISODES = 3\n",
        "\n",
        "file_path_list = [\n",
        "    FIGS_DIR / f\"{VIDEO_PREFIX_PROJECT_PPO_TRAINED}-episode-{episode_index}.mp4\"\n",
        "    for episode_index in range(NUM_EPISODES)\n",
        "]\n",
        "\n",
        "for file_path in file_path_list:\n",
        "    file_path.unlink(missing_ok=True)\n",
        "\n",
        "# env = gym.make(\"LunarLander-v3\", continuous=True, gravity=-10.0,\n",
        "#                enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "\n",
        "env = gym.make(\"LunarLander-v3\", continuous=True, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "\n",
        "\n",
        "env = gym.wrappers.RecordVideo(\n",
        "    env,\n",
        "    video_folder=str(FIGS_DIR),\n",
        "    name_prefix=VIDEO_PREFIX_PROJECT_PPO_TRAINED,\n",
        "    episode_trigger=lambda x: True,\n",
        ")\n",
        "env = gym.wrappers.RecordEpisodeStatistics(env, buffer_length=NUM_EPISODES)\n",
        "\n",
        "for episode_index in range(NUM_EPISODES):\n",
        "    episode_states, episode_actions, episode_log_prob_actions, episode_rewards, episode_values = sample_trajectory(env,actor_nn,critic_nn, max_steps = 1000) # TODO...\n",
        "\n",
        "print(f\"Episode time taken: {env.time_queue}\")\n",
        "print(f\"Episode total rewards: {env.return_queue}\")\n",
        "print(f\"Episode lengths: {env.length_queue}\")\n",
        "\n",
        "env.close()\n",
        "\n",
        "print(\"\\nSelect the episode to play here ðŸ‘‡\\n\")\n",
        "\n",
        "interact(video_selector, file_path=file_path_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2iHzDtw66Z1M"
      },
      "outputs": [],
      "source": [
        "PPO_trains_result_df.to_csv(\"./data/project_PPO_trains_result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5MVUi5l6Z1M"
      },
      "outputs": [],
      "source": [
        "g = sns.relplot(\n",
        "    x=\"num_episodes\",\n",
        "    y=\"mean_final_episode_reward\",\n",
        "    kind=\"line\",\n",
        "    hue=\"agent\",\n",
        "    estimator=None,\n",
        "    units=\"training_index\",\n",
        "    data=PPO_trains_result_df,\n",
        "    height=7,\n",
        "    aspect=2,\n",
        "    alpha=0.5,\n",
        ")\n",
        "plt.savefig(PLOTS_DIR / \"project_PPO_trains_result.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "os_mtoeW6Z1M"
      },
      "outputs": [],
      "source": [
        "mean_score_PPO = PPO_trains_result_df[\"mean_final_episode_reward\"].mean()\n",
        "\n",
        "mean_score_PPO"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DTS00zX6Z1M"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=True, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "mean_test_reward = evaluate_policy(actor_nn,env,200,500)\n",
        "env.close()\n",
        "mean_test_reward"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1CglbFf46Z1M"
      },
      "source": [
        "## Optimized Hyperparameters' Performance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKktXusm6Z1M"
      },
      "source": [
        "### DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q7i6MD9n6Z1M"
      },
      "outputs": [],
      "source": [
        "with open(\"data/project_dqn2_hyperparams.pkl\", \"rb\") as f:\n",
        "    dqn_params = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p6K2IwyG6Z1M"
      },
      "outputs": [],
      "source": [
        "dqn_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ez1OtoYD6Z1M"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
        "dqn2_trains_result_list: List[List[Union[int, float]]] = [[], [], []]\n",
        "\n",
        "for train_index in range(NUMBER_OF_TRAININGS):\n",
        "    # Instantiate required objects\n",
        "\n",
        "    q_network = LinearNet(LL_observation_dim, LL_action_number, hidden_dim=dqn_params[\"hidden_size\"], n_layer = dqn_params[\"layer_num\"]).to(device)\n",
        "\n",
        "    target_q_network = LinearNet(LL_observation_dim, LL_action_number, hidden_dim=dqn_params[\"hidden_size\"], n_layer = dqn_params[\"layer_num\"]).to(device) # The target Q-network is used to compute the target Q-values for the loss function\n",
        "\n",
        "    # Initialize the target Q-network with the same weights as the Q-network (c.f. the \"Practical tips\" section of the exercise)\n",
        "    target_q_network.load_state_dict(q_network.state_dict())\n",
        "\n",
        "    optimizer = torch.optim.AdamW(q_network.parameters(), lr=dqn_params[\"lr_start\"], amsgrad=True)\n",
        "    # lr_scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.999)\n",
        "    lr_scheduler = MinimumExponentialLR(optimizer, lr_decay=dqn_params[\"lr_decay\"], min_lr=dqn_params[\"lr_min\"])\n",
        "    loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "    epsilon_greedy = EpsilonGreedy(\n",
        "        epsilon_start=dqn_params[\"epsilon_start\"],\n",
        "        epsilon_min=dqn_params[\"epsilon_min\"],\n",
        "        epsilon_decay=dqn_params[\"epsilon_decay\"],\n",
        "        env=env,\n",
        "        q_network=q_network,\n",
        "    )\n",
        "\n",
        "    replay_buffer = ReplayBuffer(dqn_params[\"len_buffer\"])\n",
        "\n",
        "    # Train the q-network\n",
        "\n",
        "    episode_reward_list = train_dqn2_agent(\n",
        "        env,\n",
        "        q_network,\n",
        "        target_q_network,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        epsilon_greedy,\n",
        "        device,\n",
        "        lr_scheduler,\n",
        "        num_episodes=200,\n",
        "        gamma=0.9,\n",
        "        batch_size=128,\n",
        "        replay_buffer=replay_buffer,\n",
        "        target_q_network_sync_period=dqn_params[\"target_update_period\"],\n",
        "    )\n",
        "    print(np.mean(episode_reward_list))\n",
        "    dqn2_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    dqn2_trains_result_list[1].extend(episode_reward_list)\n",
        "    dqn2_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "dqn2_trains_result_df = pd.DataFrame(\n",
        "    np.array(dqn2_trains_result_list).T,\n",
        "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
        ")\n",
        "dqn2_trains_result_df[\"agent\"] = \"DQN v2\"\n",
        "\n",
        "# Save the action-value estimation function\n",
        "\n",
        "torch.save(q_network, MODELS_DIR / \"project_new_dqn2_q_network.pth\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "16yuXzu66Z1M"
      },
      "outputs": [],
      "source": [
        "dqn2_trains_result_df.to_csv(\"./data/project_new_dqn2_train_result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HufHveIT6Z1M"
      },
      "outputs": [],
      "source": [
        "g = sns.relplot(\n",
        "    x=\"num_episodes\",\n",
        "    y=\"mean_final_episode_reward\",\n",
        "    kind=\"line\",\n",
        "    hue=\"agent\",\n",
        "    estimator=None,\n",
        "    units=\"training_index\",\n",
        "    data=dqn2_trains_result_df,\n",
        "    height=7,\n",
        "    aspect=2,\n",
        "    alpha=0.5,\n",
        ")\n",
        "plt.savefig(PLOTS_DIR / \"project_new_dqn2_trains_result.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0fyDVre6Z1N"
      },
      "outputs": [],
      "source": [
        "mean_score_dqn2 = dqn2_trains_result_df[\"mean_final_episode_reward\"].mean()\n",
        "mean_score_dqn2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x4ZaBw2G6Z1N"
      },
      "outputs": [],
      "source": [
        "score_dqn2 = dqn2_trains_result_df[[\"num_episodes\", \"mean_final_episode_reward\"]].groupby(\"num_episodes\").mean().max()\n",
        "score_dqn2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pECh7huE6Z1N"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "dqn_reward_list = test_q_network_agent(env, q_network, num_episode=200)\n",
        "dqn_reward_index = np.arange(200)\n",
        "env.close()\n",
        "print(np.mean(dqn_reward_list), np.max(dqn_reward_list))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_1yRoxe6Z1N"
      },
      "source": [
        "### REINFORCE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b41ei7A26Z1N"
      },
      "outputs": [],
      "source": [
        "with open(\"data/project_reinforce_hyperparams.pkl\", \"rb\") as f:\n",
        "    reinforce_params = pickle.load(f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fIX2n_l26Z1N"
      },
      "outputs": [],
      "source": [
        "reinforce_params"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_c2NPRdi6Z1N"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "\n",
        "NUMBER_OF_TRAININGS = DEFAULT_NUMBER_OF_TRAININGS  # Change the default (global) value here if you want a specific number of trainings for this exercise\n",
        "reinforce_trains_result_list: List[List[Union[int, float]]] = [[], [], []]\n",
        "\n",
        "for train_index in range(NUMBER_OF_TRAININGS):\n",
        "    # Train the agent\n",
        "    reinforce_policy_nn, episode_reward_list = new_train_reinforce_discrete(\n",
        "        env=env,\n",
        "        hidden_dim = reinforce_params[\"hidden_size\"],\n",
        "        n_layers = reinforce_params[\"layer_num\"],\n",
        "        num_train_episodes=200,\n",
        "        num_test_per_episode=5,\n",
        "        max_episode_duration=500,\n",
        "        lr_start=reinforce_params[\"lr_start\"],\n",
        "        lr_decay=reinforce_params[\"lr_decay\"],\n",
        "        min_lr = reinforce_params[\"lr_min\"]\n",
        "    )\n",
        "\n",
        "    reinforce_trains_result_list[0].extend(range(len(episode_reward_list)))\n",
        "    reinforce_trains_result_list[1].extend(episode_reward_list)\n",
        "    reinforce_trains_result_list[2].extend([train_index for _ in episode_reward_list])\n",
        "\n",
        "reinforce_trains_result_df = pd.DataFrame(\n",
        "    np.array(reinforce_trains_result_list).T,\n",
        "    columns=[\"num_episodes\", \"mean_final_episode_reward\", \"training_index\"],\n",
        ")\n",
        "reinforce_trains_result_df[\"agent\"] = \"REINFORCE\"\n",
        "\n",
        "# Save the action-value estimation function of the last train\n",
        "\n",
        "torch.save(reinforce_policy_nn, MODELS_DIR / \"project_new_reinforce_policy_network.pth\")\n",
        "\n",
        "env.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TvKv3aAv6Z1N"
      },
      "outputs": [],
      "source": [
        "reinforce_trains_result_df.to_csv(\"./data/project_new_reinforce_trains_result.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sW5j23MQ6Z1N"
      },
      "outputs": [],
      "source": [
        "g = sns.relplot(\n",
        "    x=\"num_episodes\",\n",
        "    y=\"mean_final_episode_reward\",\n",
        "    kind=\"line\",\n",
        "    hue=\"agent\",\n",
        "    estimator=None,\n",
        "    units=\"training_index\",\n",
        "    data=reinforce_trains_result_df,\n",
        "    height=7,\n",
        "    aspect=2,\n",
        "    alpha=0.5,\n",
        ")\n",
        "plt.savefig(PLOTS_DIR / \"project_new_reinforce_trains_result.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4FmHdbCj6Z1N"
      },
      "outputs": [],
      "source": [
        "mean_score_reinforce = reinforce_trains_result_df[\"mean_final_episode_reward\"].mean()\n",
        "\n",
        "mean_score_reinforce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6y4TlQj6Z1N"
      },
      "outputs": [],
      "source": [
        "env = gym.make(\"LunarLander-v3\", continuous=False, gravity=-10.0,\n",
        "               enable_wind=False, wind_power=15.0, turbulence_power=1.5, render_mode = \"rgb_array\")\n",
        "mean_test_reward = avg_return_on_multiple_episodes(env,reinforce_policy_nn,200,500)\n",
        "env.close()\n",
        "mean_test_reward"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCVbpwOT6Z1N"
      },
      "outputs": [],
      "source": [
        "all_trains_result_df = pd.concat(\n",
        "    [\n",
        "        dqn2_trains_result_df,\n",
        "        reinforce_trains_result_df,\n",
        "    ]\n",
        ")\n",
        "g = sns.relplot(\n",
        "    x=\"num_episodes\",\n",
        "    y=\"mean_final_episode_reward\",\n",
        "    kind=\"line\",\n",
        "    hue=\"agent\",\n",
        "    data=all_trains_result_df,\n",
        "    height=7,\n",
        "    aspect=2,\n",
        ")\n",
        "plt.savefig(PLOTS_DIR / \"project_new_trains_result_agg.png\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "kt9cPWYH6Z1G"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
